{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e162344-b805-48b2-91d5-e168a3fd36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from tslearn.metrics import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed44bfa-de85-44bb-8aa2-f6d0187ff547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set font configuration for English display\n",
    "plt.rcParams[\"font.family\"] = [\"DejaVu Sans\", \"Arial\", \"sans-serif\"]\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Ensure minus sign displays correctly\n",
    "\n",
    "# Data folder path\n",
    "data_folder = r\"D:\\country_datatable\\2003-2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22521bb-7767-4189-b293-a0e9ad8b0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_country_data(folder_path):\n",
    "    \"\"\"Load and process country time series data\"\"\"\n",
    "    # Get all CSV files (sorted by year)\n",
    "    csv_files = sorted(\n",
    "        [f for f in os.listdir(folder_path) if f.endswith('.csv')],\n",
    "        key=lambda x: int(x.split('.')[0])\n",
    "    )\n",
    "\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in folder {folder_path}\")\n",
    "\n",
    "    # Extract year information\n",
    "    years = [int(f.split('.')[0]) for f in csv_files]\n",
    "    print(f\"Years found in data: {years}\")\n",
    "\n",
    "    # Read the first file to get country list and indicator names\n",
    "    first_file = os.path.join(folder_path, csv_files[0])\n",
    "    df_first = pd.read_csv(first_file)\n",
    "    all_countries = df_first['COUNTRY'].unique()\n",
    "    indicator_names = df_first.columns[2:].tolist()  # Columns from 3rd are indicators\n",
    "    print(f\"Total {len(all_countries)} countries/regions, {len(indicator_names)} indicators\")\n",
    "\n",
    "    # Build time series data for each country (country -> [time points, indicator values])\n",
    "    country_data = {}\n",
    "    country_iso_map = {}  # Store mapping between country and ISO code\n",
    "\n",
    "    for country in all_countries:\n",
    "        # Initialize country data (number of years x number of indicators)\n",
    "        country_series = np.zeros((len(years), len(indicator_names)))\n",
    "        country_series[:] = np.nan  # Initialize with NaN\n",
    "\n",
    "        # Extract ISO code (assuming the same for all years)\n",
    "        iso_code = df_first[df_first['COUNTRY'] == country]['ISO'].iloc[0]\n",
    "        country_iso_map[country] = iso_code\n",
    "\n",
    "        for i, csv_file in enumerate(csv_files):\n",
    "            file_path = os.path.join(folder_path, csv_file)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Extract data for current country\n",
    "            country_df = df[df['COUNTRY'] == country]\n",
    "\n",
    "            if not country_df.empty:\n",
    "                # Extract indicator values and store in corresponding year position\n",
    "                indicators = country_df.iloc[0, 2:].values\n",
    "                country_series[i] = indicators\n",
    "\n",
    "        country_data[country] = country_series\n",
    "\n",
    "    return country_data, years, indicator_names, country_iso_map\n",
    "\n",
    "\n",
    "def preprocess_data(country_data):\n",
    "    \"\"\"Preprocess time series data by filling all missing values with 0\"\"\"\n",
    "    processed_series = []\n",
    "    countries = []\n",
    "\n",
    "    for country, series in country_data.items():\n",
    "        # Fill all missing values with 0\n",
    "        filled_series = np.nan_to_num(series, nan=0.0)\n",
    "        processed_series.append(filled_series)\n",
    "        countries.append(country)\n",
    "\n",
    "    print(f\"Number of countries after processing: {len(countries)}\")\n",
    "    return np.array(processed_series), countries\n",
    "\n",
    "\n",
    "def find_optimal_clusters(data):\n",
    "    \"\"\"Calculate silhouette scores for cluster numbers 2, 3, 4, 5, 6\"\"\"\n",
    "    silhouette_scores = []\n",
    "    k_values = [2, 3, 4, 5, 6]\n",
    "\n",
    "    for k in k_values:\n",
    "        kmeans = TimeSeriesKMeans(\n",
    "            n_clusters=k,\n",
    "            metric=\"dtw\",\n",
    "            max_iter=100,\n",
    "            random_state=42\n",
    "        )\n",
    "        labels = kmeans.fit_predict(data)\n",
    "\n",
    "        # Calculate DTW distance matrix (for silhouette score)\n",
    "        distance_matrix = np.zeros((len(data), len(data)))\n",
    "\n",
    "        # Use tslearn's dtw function to calculate distances directly\n",
    "        for i in range(len(data)):\n",
    "            for j in range(i + 1, len(data)):\n",
    "                # Ensure correct data format\n",
    "                dist = dtw(data[i], data[j])\n",
    "                distance_matrix[i, j] = dist\n",
    "                distance_matrix[j, i] = dist\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        score = silhouette_score(distance_matrix, labels, metric=\"precomputed\")\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"Number of clusters k={k}, Silhouette Score: {score:.4f}\")\n",
    "\n",
    "    # Plot silhouette scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot([2, 3, 4, 5, 6], silhouette_scores, 'o-')\n",
    "    plt.xlabel('Number of clusters k')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Scores for Different Cluster Numbers')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('silhouette_scores.png')\n",
    "    plt.close()\n",
    "\n",
    "    return k_values, silhouette_scores\n",
    "\n",
    "\n",
    "def perform_clustering(data, n_clusters):\n",
    "    \"\"\"Perform time series clustering\"\"\"\n",
    "    # Standardize data\n",
    "    scaler = TimeSeriesScalerMeanVariance()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    # Perform K-means clustering using DTW\n",
    "    kmeans = TimeSeriesKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        metric=\"dtw\",\n",
    "        max_iter=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all CPU cores to speed up computation\n",
    "    )\n",
    "\n",
    "    labels = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "    # Calculate silhouette score\n",
    "    distance_matrix = np.zeros((len(scaled_data), len(scaled_data)))\n",
    "\n",
    "    # Use tslearn's dtw function to calculate distances directly\n",
    "    for i in range(len(scaled_data)):\n",
    "        for j in range(i + 1, len(scaled_data)):\n",
    "            dist = dtw(scaled_data[i], scaled_data[j])\n",
    "            distance_matrix[i, j] = dist\n",
    "            distance_matrix[j, i] = dist\n",
    "\n",
    "    silhouette_avg = silhouette_score(distance_matrix, labels, metric=\"precomputed\")\n",
    "    print(f\"Final clustering results (k={n_clusters}): Silhouette Score = {silhouette_avg:.4f}\")\n",
    "\n",
    "    return kmeans, scaled_data, labels\n",
    "\n",
    "\n",
    "def visualize_clusters(kmeans, data, labels, countries, years, indicator_names, country_iso_map, k):\n",
    "    \"\"\"Visualize clustering results and save clustering results to CSV\"\"\"\n",
    "    n_clusters = len(kmeans.cluster_centers_)\n",
    "\n",
    "    # Use t-SNE for dimensionality reduction to visualize high-dimensional clustering results\n",
    "    n_samples, n_timesteps, n_features = data.shape\n",
    "    data_2d = data.reshape(n_samples, n_timesteps * n_features)\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=10)\n",
    "    data_tsne = tsne.fit_transform(data_2d)\n",
    "\n",
    "    # Plot clustering scatter plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    for i in range(n_clusters):\n",
    "        cluster_indices = np.where(labels == i)[0]\n",
    "        plt.scatter(\n",
    "            data_tsne[cluster_indices, 0],\n",
    "            data_tsne[cluster_indices, 1],\n",
    "            label=f'Cluster {i} ({len(cluster_indices)} countries/regions)',\n",
    "            alpha=0.7, s=80\n",
    "        )\n",
    "\n",
    "        # Add labels for some example countries in each cluster\n",
    "        for idx in cluster_indices[:3]:\n",
    "            plt.annotate(\n",
    "                countries[idx],\n",
    "                (data_tsne[idx, 0], data_tsne[idx, 1]),\n",
    "                fontsize=9,\n",
    "                alpha=0.8\n",
    "            )\n",
    "\n",
    "    plt.title(f'Country Clustering Results Based on All Indicators (k={k}, t-SNE Dimensionality Reduction)')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'country_clusters_k{k}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Visualize the trend changes of cluster center indicators\n",
    "    plt.figure(figsize=(15, 5 * n_clusters))\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        plt.subplot(n_clusters, 1, i + 1)\n",
    "        cluster_center = kmeans.cluster_centers_[i]\n",
    "\n",
    "        # Select the 3 indicators with the most significant changes for visualization\n",
    "        variances = np.var(cluster_center, axis=0)\n",
    "        top_indices = np.argsort(variances)[-3:]\n",
    "\n",
    "        for j in top_indices:\n",
    "            plt.plot(years, cluster_center[:, j],\n",
    "                     label=indicator_names[j], linewidth=2)\n",
    "\n",
    "        plt.title(f'Key Indicator Trends for Cluster {i}')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Standardized Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'cluster_trends_k{k}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Save clustering results to CSV\n",
    "    cluster_results = pd.DataFrame({\n",
    "        'COUNTRY': countries,\n",
    "        'ISO': [country_iso_map[country] for country in countries],\n",
    "        'CLUSTER': labels\n",
    "    })\n",
    "\n",
    "    csv_path = f'country_clusters_k{k}.csv'\n",
    "    cluster_results.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Clustering results saved to {csv_path}\")\n",
    "\n",
    "    # Output clustering results table\n",
    "    cluster_groups = {}\n",
    "    for i in range(n_clusters):\n",
    "        cluster_countries = cluster_results[cluster_results['CLUSTER'] == i]['COUNTRY'].tolist()\n",
    "        cluster_groups[i] = cluster_countries\n",
    "\n",
    "    with open(f'cluster_results_k{k}.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Clustering Results (k={k}):\\n\\n\")\n",
    "        for i, countries_list in cluster_groups.items():\n",
    "            f.write(f\"Cluster {i} ({len(countries_list)} countries/regions):\\n\")\n",
    "            f.write(\", \".join(countries_list) + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Clustering details saved to cluster_results_k{k}.txt\")\n",
    "    print(f\"Visualization charts saved to country_clusters_k{k}.png and cluster_trends_k{k}.png\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load data\n",
    "        print(\"Loading data...\")\n",
    "        country_data, years, indicator_names, country_iso_map = load_country_data(data_folder)\n",
    "\n",
    "        # Preprocess data (fill missing values with 0)\n",
    "        print(\"Preprocessing data...\")\n",
    "        processed_data, valid_countries = preprocess_data(country_data)\n",
    "        print(f\"Processed data shape: {processed_data.shape}\")\n",
    "\n",
    "        # Calculate silhouette scores for different cluster numbers\n",
    "        print(\"Calculating silhouette scores for different cluster numbers...\")\n",
    "        k_values, silhouette_scores = find_optimal_clusters(processed_data)\n",
    "\n",
    "        # Perform clustering and visualize results for each cluster number k\n",
    "        for k in k_values:\n",
    "            print(f\"\\n===== Starting clustering analysis (k={k}) =====\")\n",
    "            # Perform clustering\n",
    "            kmeans, scaled_data, labels = perform_clustering(processed_data, k)\n",
    "\n",
    "            # Visualize clustering results and save to CSV\n",
    "            visualize_clusters(kmeans, scaled_data, labels, valid_countries, years, indicator_names, country_iso_map, k)\n",
    "\n",
    "        print(\"\\nAll clustering analyses completed!\")\n",
    "        print(\"Silhouette Score Summary:\")\n",
    "        for k, score in zip(k_values, silhouette_scores):\n",
    "            print(f\"k={k}: Silhouette Score = {score:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during program execution: {e}\")\n",
    "        # Print detailed error stack trace for debugging\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
